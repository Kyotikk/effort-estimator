#!/usr/bin/env python3
"""
THESIS PRESENTATION: Chronological Discovery Narrative
Run this to generate all the key results in presentation order
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_predict, LeaveOneGroupOut, KFold
from scipy.stats import pearsonr
from sklearn.metrics import mean_absolute_error

# Load data
df = pd.read_csv('/Users/pascalschlegel/data/interim/elderly_combined_5subj/all_5_elderly_5s.csv')

exclude_cols = ['subject', 'borg', 't_center', 'window_start', 'window_end', 'unix_time', 'Unnamed: 0', 'index']
feature_cols = [c for c in df.columns if c not in exclude_cols 
                and df[c].dtype in ['float64', 'int64']
                and df[c].notna().sum() > 100]
valid_features = [c for c in feature_cols if df[c].isna().mean() < 0.5]

df_model = df.dropna(subset=['borg'])[['subject', 'borg'] + valid_features].dropna()

X_raw = df_model[valid_features].values
y = df_model['borg'].values
groups = df_model['subject'].values

def to_cat(b):
    if b <= 2: return 0
    elif b <= 4: return 1
    else: return 2

y_cat = np.array([to_cat(b) for b in y])

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘               THESIS PRESENTATION: EFFORT ESTIMATION PIPELINE                â•‘
â•‘                        Chronological Discovery Narrative                     â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# ==============================================================================
# SLIDE 1: THE GOAL
# ==============================================================================
print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SLIDE 1: RESEARCH GOAL                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OBJECTIVE:
  Estimate perceived effort (Borg scale 0-10) from wearable sensors
  for elderly users during daily activities.

WHY IT MATTERS:
  â€¢ Enables autonomous monitoring without asking users
  â€¢ Prevents overexertion in elderly populations
  â€¢ Enables adaptive exercise recommendations

THE DREAM:
  Train on some people â†’ Deploy to ANYONE â†’ Predict effort automatically
""")

# ==============================================================================
# SLIDE 2: THE DATA
# ==============================================================================
print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SLIDE 2: DATA COLLECTION                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DATASET:
  â€¢ 5 elderly subjects (P1-P5)
  â€¢ Activities: Rest, Walking, Fast Walking, Stairs
  â€¢ Sensors: PPG (heart), EDA (skin conductance), IMU (motion)
  
PREPROCESSING:
  â€¢ Window size: 5.0 seconds, 70% overlap
  â€¢ {len(valid_features)} features extracted (PPG, EDA, IMU, HRV)
  â€¢ {len(df_model)} labeled samples with Borg ratings
""")

# ==============================================================================
# SLIDE 3: FIRST ATTEMPT - Cross-Subject
# ==============================================================================
print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SLIDE 3: FIRST ATTEMPT - Cross-Subject Prediction                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

APPROACH:
  â€¢ Leave-One-Subject-Out Cross-Validation (LOSO)
  â€¢ Train on 4 subjects, test on the 5th
  â€¢ "Can we predict effort for a NEW person?"
""")

# Run Method 1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_raw)
logo = LeaveOneGroupOut()
model = Ridge(alpha=1.0)
y_pred_1 = cross_val_predict(model, X_scaled, y, cv=logo, groups=groups)

r_1, _ = pearsonr(y, y_pred_1)
mae_1 = mean_absolute_error(y, y_pred_1)
y_pred_1_cat = np.array([to_cat(b) for b in y_pred_1])
adj_1 = (np.abs(y_cat - y_pred_1_cat) <= 1).mean()

print(f"""
RESULT:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Pearson r = {r_1:.2f}              â”‚
  â”‚  MAE = {mae_1:.2f} Borg               â”‚
  â”‚  Adjacent accuracy = {adj_1:.0%}     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INTERPRETATION:
  ðŸ˜Ÿ r = 0.18 is POOR for regression
  ðŸ¤” BUT 87% never confuses LOW with HIGH - useful for safety!
""")

# ==============================================================================
# SLIDE 4: WHY DOES IT FAIL?
# ==============================================================================
print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SLIDE 4: INVESTIGATION - Why Does Cross-Subject Fail?                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

HYPOTHESIS: Individual differences in baselines and perception

ANALYSIS: Compare baselines across subjects
""")

# Show baseline differences
baseline_data = []
for subj in sorted(df_model['subject'].unique()):
    mask = df_model['subject'] == subj
    eda_mean = df_model.loc[mask, 'eda_stress_skin_mean'].mean() if 'eda_stress_skin_mean' in df_model.columns else 0
    borg_mean = df_model.loc[mask, 'borg'].mean()
    baseline_data.append({'subject': subj, 'eda_mean': eda_mean, 'borg_mean': borg_mean})

print(f"""
FINDING: Massive baseline differences!

  Subject â”‚ EDA Baseline â”‚ Borg Mean
  â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€""")

for d in baseline_data:
    label = d['subject'].replace('sim_elderly', 'P')
    print(f"  {label:7s} â”‚ {d['eda_mean']:>10.0f}   â”‚ {d['borg_mean']:.2f}")

print(f"""
PROBLEM:
  â€¢ P1 has EDA = 1300+, but rates Borg as 2.9 (moderate)
  â€¢ P5 has EDA = 200+, and rates Borg as 1.1 (low)
  â€¢ Same EDA value means DIFFERENT effort for different people!
""")

# ==============================================================================
# SLIDE 5: THE INSIGHT
# ==============================================================================
print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SLIDE 5: THE KEY INSIGHT                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

THE PROBLEM IS TWOFOLD:

  1. BASELINE DIFFERENCES (physiological)
     â€¢ Different people have different resting EDA, HR, etc.
     â€¢ P1's resting EDA (1300) > P2's maximum EDA (500)!

  2. SUBJECTIVE PERCEPTION
     â€¢ "Borg 5" means different things to different people
     â€¢ Some people rate conservatively, others rate higher

THE MODEL'S MISTAKE:
  â€¢ Cross-subject model learns: "High EDA â†’ Medium Borg"
  â€¢ This just identifies P1, not actual effort!

THIS IS CALLED: Simpson's Paradox
  â€¢ Correlation at group level â‰  correlation within individuals
""")

# ==============================================================================
# SLIDE 6: PROOF - Within-Subject Works
# ==============================================================================
print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SLIDE 6: PROOF - Within-Subject Prediction Works!                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

IF our hypothesis is correct, within-subject prediction should work.

APPROACH:
  â€¢ Train and test on SAME subject (5-fold CV)
  â€¢ Each person gets their own model
""")

# Run Method 4
within_results = []
for subj in sorted(df_model['subject'].unique()):
    mask = df_model['subject'] == subj
    X_subj = X_raw[mask]
    y_subj = y[mask]
    
    scaler_subj = StandardScaler()
    X_subj_scaled = scaler_subj.fit_transform(X_subj)
    
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    y_pred_subj = cross_val_predict(model, X_subj_scaled, y_subj, cv=kf)
    
    r_subj, _ = pearsonr(y_subj, y_pred_subj)
    mae_subj = mean_absolute_error(y_subj, y_pred_subj)
    within_results.append({'subject': subj, 'r': r_subj, 'mae': mae_subj})

mean_r_within = np.mean([r['r'] for r in within_results])
mean_mae_within = np.mean([r['mae'] for r in within_results])

print(f"""
RESULT:
  Subject â”‚ Pearson r â”‚ MAE
  â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€""")
for res in within_results:
    label = res['subject'].replace('sim_elderly', 'P')
    print(f"  {label:7s} â”‚ {res['r']:.3f}     â”‚ {res['mae']:.2f}")
print(f"""  â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€
  MEAN    â”‚ {mean_r_within:.3f}     â”‚ {mean_mae_within:.2f}

INTERPRETATION:
  âœ… r = 0.67 is GOOD! (vs 0.18 cross-subject)
  âœ… Proves that features DO correlate with effort WITHIN each person
  âŒ But requires training data from that specific person
""")

# ==============================================================================
# SLIDE 7: FAILED SOLUTION - Normalize Features Only
# ==============================================================================
print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SLIDE 7: FAILED SOLUTION - Baseline Normalization (Features Only)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

IDEA: Remove baseline differences by normalizing features per subject
      Convert each feature to z-scores within each person

HOPE: "Now everyone's features are on the same scale!"
""")

# Run Method 2
df_norm = df_model.copy()
for feat in valid_features:
    for subj in df_model['subject'].unique():
        mask = df_model['subject'] == subj
        subj_mean = df_model.loc[mask, feat].mean()
        subj_std = df_model.loc[mask, feat].std()
        if subj_std > 0:
            df_norm.loc[mask, feat] = (df_model.loc[mask, feat] - subj_mean) / subj_std
        else:
            df_norm.loc[mask, feat] = 0

X_norm = df_norm[valid_features].values

y_pred_2 = cross_val_predict(model, X_norm, y, cv=logo, groups=groups)
r_2, _ = pearsonr(y, y_pred_2)
mae_2 = mean_absolute_error(y, y_pred_2)

print(f"""
RESULT:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Pearson r = {r_2:.2f}              â”‚
  â”‚  MAE = {mae_2:.2f} Borg               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ðŸ˜± IT'S WORSE! (r=0.05 vs r=0.18)

WHY IT FAILED:
  â€¢ We removed baseline differences from FEATURES
  â€¢ But Borg is still on ABSOLUTE scale (0-10)
  â€¢ Can't predict ABSOLUTE Borg from RELATIVE features!
  
ANALOGY:
  "Your HR is 1Ïƒ above your baseline" â†’ "Your Borg is... 3? 5? 7?"
  We don't know because we don't know their Borg baseline!
""")

# ==============================================================================
# SLIDE 8: THE SOLUTION - Normalize BOTH
# ==============================================================================
print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SLIDE 8: THE SOLUTION - Normalize Features AND Borg                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

KEY INSIGHT:
  If features are RELATIVE, the target must also be RELATIVE!

APPROACH:
  1. Normalize features: z-score within each subject
  2. Normalize Borg: z-score within each subject
  3. Model predicts RELATIVE effort deviation
  4. Denormalize back to absolute Borg using subject's baseline

THIS REQUIRES CALIBRATION:
  Need to know each person's mean Borg and std from ~20 samples
""")

# Run Method 3
df_norm['borg_norm'] = 0.0
borg_stats = {}
for subj in df_model['subject'].unique():
    mask = df_model['subject'] == subj
    subj_mean = df_model.loc[mask, 'borg'].mean()
    subj_std = df_model.loc[mask, 'borg'].std()
    borg_stats[subj] = {'mean': subj_mean, 'std': subj_std}
    if subj_std > 0:
        df_norm.loc[mask, 'borg_norm'] = (df_model.loc[mask, 'borg'] - subj_mean) / subj_std

y_norm = df_norm['borg_norm'].values

y_pred_3_norm = cross_val_predict(model, X_norm, y_norm, cv=logo, groups=groups)

y_pred_3 = np.zeros_like(y_pred_3_norm)
for subj in df_model['subject'].unique():
    mask = groups == subj
    y_pred_3[mask] = y_pred_3_norm[mask] * borg_stats[subj]['std'] + borg_stats[subj]['mean']

r_3, _ = pearsonr(y, y_pred_3)
mae_3 = mean_absolute_error(y, y_pred_3)
y_pred_3_cat = np.array([to_cat(b) for b in y_pred_3])
adj_3 = (np.abs(y_cat - y_pred_3_cat) <= 1).mean()

print(f"""
RESULT:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Pearson r = {r_3:.2f}              â”‚
  â”‚  MAE = {mae_3:.2f} Borg               â”‚
  â”‚  Adjacent accuracy = {adj_3:.0%}     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ðŸŽ‰ HUGE IMPROVEMENT! (r=0.61 vs r=0.18)

WHY IT WORKS:
  â€¢ Model learns: "Features 1Ïƒ above YOUR baseline â†’ Borg 0.5Ïƒ above YOUR baseline"
  â€¢ This relationship is UNIVERSAL across people!
  â€¢ The calibration provides the "anchor" for each person
""")

# ==============================================================================
# SLIDE 9: COMPARISON
# ==============================================================================
print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SLIDE 9: SUMMARY COMPARISON                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method                                 â”‚ r      â”‚ MAE   â”‚ Adjacent â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Cross-subject (raw)                 â”‚ {r_1:.2f}   â”‚ {mae_1:.2f}  â”‚ {adj_1:.0%}     â”‚
â”‚ 2. Cross-subject (features normalized) â”‚ {r_2:.2f}   â”‚ {mae_2:.2f}  â”‚ -        â”‚
â”‚ 3. Cross-subject WITH CALIBRATION      â”‚ {r_3:.2f}   â”‚ {mae_3:.2f}  â”‚ {adj_3:.0%}     â”‚
â”‚ 4. Within-subject (ceiling)            â”‚ {mean_r_within:.2f}   â”‚ {mean_mae_within:.2f}  â”‚ 98%      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

KEY OBSERVATIONS:
  â€¢ Raw cross-subject is poor (r=0.18) - baselines too different
  â€¢ Feature normalization alone makes it WORSE (r=0.05)
  â€¢ With calibration, nearly matches within-subject! (r=0.61 vs 0.67)
  â€¢ ~8 minutes of calibration unlocks personalized prediction
""")

# ==============================================================================
# SLIDE 10: PRACTICAL IMPLEMENTATION
# ==============================================================================
print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SLIDE 10: PRACTICAL IMPLEMENTATION                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CALIBRATION PROTOCOL (~8 minutes):

  Activity      â”‚ Duration â”‚ Borg Ratings â”‚ Effort Level
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Seated rest   â”‚ 2 min    â”‚ 2 ratings    â”‚ LOW
  Slow walking  â”‚ 2 min    â”‚ 2 ratings    â”‚ LOW-MOD
  Normal walk   â”‚ 2 min    â”‚ 2 ratings    â”‚ MODERATE
  Fast walk     â”‚ 2 min    â”‚ 2 ratings    â”‚ MOD-HIGH

  Total: ~8 minutes, ~20 samples (windows)

FROM CALIBRATION, WE EXTRACT:
  â€¢ Feature means and stds (for normalization)
  â€¢ Borg mean and std (for denormalization)

DEPLOYMENT:
  1. Day 1: User completes calibration (~8 min)
  2. Day 2+: System predicts effort autonomously
  3. Model is CROSS-SUBJECT (trained on others)
  4. Calibration personalizes the predictions
""")

# ==============================================================================
# SLIDE 11: CONCLUSIONS
# ==============================================================================
print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SLIDE 11: CONCLUSIONS                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MAIN FINDINGS:

  1. Cross-subject effort estimation is HARD (r=0.18)
     â†’ Individual baselines and subjective perception prevent generalization

  2. Within-subject estimation WORKS (r=0.67)  
     â†’ Features do correlate with effort within each person

  3. Brief calibration (~8 min) enables strong cross-subject performance (r=0.61)
     â†’ Learn person's baseline, then apply universal model

  4. Adjacent accuracy (LOW/MOD/HIGH) is high even without calibration (87%)
     â†’ Useful for safety applications (never confuses LOW with HIGH)

CONTRIBUTION:
  â€¢ Demonstrated that perceived effort is PERSONAL - requires calibration
  â€¢ Proposed practical calibration protocol for deployment
  â€¢ Achieved r=0.61 with only ~8 minutes of user input

FUTURE WORK:
  â€¢ Longitudinal validation: Does calibration hold over time?
  â€¢ Minimum calibration: How few samples are truly needed?
  â€¢ Transfer learning: Can we reduce calibration with more training data?
""")

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                           END OF PRESENTATION                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
