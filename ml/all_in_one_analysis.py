#!/usr/bin/env python3
"""
All-in-One Model Analysis and Visualization Script
Generates comprehensive plots: train/test comparison, metrics, features, residuals
"""

import pandas as pd
import numpy as np
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pickle
import warnings
warnings.filterwarnings('ignore')

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.facecolor'] = 'white'

def load_model_and_prepare():
    """Load model and prepare data"""
    output_dir = Path("/Users/pascalschlegel/data/interim/parsingsim3/sim_elderly3/effort_estimation_output/parsingsim3_sim_elderly3/xgboost_models")
    
    # Load model and scaler
    model = xgb.XGBRegressor()
    model.load_model(str(output_dir / "xgboost_borg_10.0s.json"))
    
    with open(output_dir / "scaler_10.0s.pkl", "rb") as f:
        scaler = pickle.load(f)
    
    # Load data
    fused_path = Path("/Users/pascalschlegel/data/interim/parsingsim3/sim_elderly3/effort_estimation_output/parsingsim3_sim_elderly3/fused_aligned_10.0s.csv")
    df = pd.read_csv(fused_path)
    df_labeled = df.dropna(subset=["borg"]).copy()
    
    # Filter features
    skip_cols = {
        "window_id", "start_idx", "end_idx", "valid",
        "t_start", "t_center", "t_end", "n_samples", "win_sec",
        "modality", "subject", "borg",
    }
    
    def is_metadata(col):
        if col in skip_cols:
            return True
        if col.endswith("_r") or any(col.endswith(f"_r.{i}") for i in range(1, 10)):
            return True
        return False
    
    feature_cols = [col for col in df_labeled.columns if not is_metadata(col)]
    X = df_labeled[feature_cols].values
    y = df_labeled["borg"].values
    
    # Feature selection (top 100 by correlation)
    correlations = np.array([np.corrcoef(X[:, i], y)[0, 1] for i in range(X.shape[1])])
    correlations = np.abs(np.nan_to_num(correlations, nan=0))
    top_indices = np.argsort(correlations)[-100:][::-1]
    selected_cols = [feature_cols[i] for i in top_indices]
    
    X_selected = X[:, top_indices]
    
    # Split data with same seed as training
    X_train, X_test, y_train, y_test = train_test_split(
        X_selected, y, test_size=0.2, random_state=42
    )
    
    # Scale - fit on training, transform both
    scaler_new = StandardScaler()
    X_train_scaled = scaler_new.fit_transform(X_train)
    X_test_scaled = scaler_new.transform(X_test)
    
    # Predictions
    y_train_pred = model.predict(X_train_scaled)
    y_test_pred = model.predict(X_test_scaled)
    
    # Load actual metrics from saved file for display (in case of numerical differences)
    import json
    metrics_path = output_dir / "evaluation_metrics.json"
    saved_metrics = None
    if metrics_path.exists():
        with open(metrics_path) as f:
            saved_data = json.load(f)
            if "10.0s" in saved_data:
                saved_metrics = saved_data["10.0s"]
    
    return {
        'model': model,
        'scaler': scaler,
        'X_train_scaled': X_train_scaled,
        'X_test_scaled': X_test_scaled,
        'y_train': y_train,
        'y_test': y_test,
        'y_train_pred': y_train_pred,
        'y_test_pred': y_test_pred,
        'selected_cols': selected_cols,
        'feature_cols': feature_cols,
        'correlations': correlations,
        'saved_metrics': saved_metrics,
    }

def plot_train_vs_test_with_diagonal(data):
    """Train vs Test with RED diagonal perfect prediction line"""
    y_train = data['y_train']
    y_test = data['y_test']
    y_train_pred = data['y_train_pred']
    y_test_pred = data['y_test_pred']
    
    r2_train = r2_score(y_train, y_train_pred)
    r2_test = r2_score(y_test, y_test_pred)
    mae_train = mean_absolute_error(y_train, y_train_pred)
    mae_test = mean_absolute_error(y_test, y_test_pred)
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 7))
    
    # Train set
    ax = axes[0]
    ax.scatter(y_train, y_train_pred, s=80, alpha=0.6, color='#1f77b4', 
              edgecolors='black', linewidth=0.5, label='Training samples')
    
    # RED perfect prediction line
    min_val = min(y_train.min(), y_train_pred.min())
    max_val = max(y_train.max(), y_train_pred.max())
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=3, label='Perfect Prediction', zorder=5)
    
    ax.set_xlabel('Actual Borg Rating', fontsize=12, fontweight='bold')
    ax.set_ylabel('Predicted Borg Rating', fontsize=12, fontweight='bold')
    ax.set_title(f'Training Set (n={len(y_train)})\nRÂ² = {r2_train:.4f} | MAE = {mae_train:.4f}', 
                fontsize=13, fontweight='bold')
    ax.legend(fontsize=11, loc='upper left')
    ax.grid(True, alpha=0.3)
    ax.set_aspect('equal', adjustable='box')
    
    # Test set
    ax = axes[1]
    scatter = ax.scatter(y_test, y_test_pred, s=100, alpha=0.6, 
                        c=np.abs(y_test - y_test_pred), cmap='RdYlGn_r',
                        edgecolors='black', linewidth=0.5, label='Test samples')
    
    # RED perfect prediction line
    min_val = min(y_test.min(), y_test_pred.min())
    max_val = max(y_test.max(), y_test_pred.max())
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=3, label='Perfect Prediction', zorder=5)
    
    ax.set_xlabel('Actual Borg Rating', fontsize=12, fontweight='bold')
    ax.set_ylabel('Predicted Borg Rating', fontsize=12, fontweight='bold')
    ax.set_title(f'Test Set (n={len(y_test)})\nRÂ² = {r2_test:.4f} | MAE = {mae_test:.4f}', 
                fontsize=13, fontweight='bold')
    ax.legend(fontsize=11, loc='upper left')
    ax.grid(True, alpha=0.3)
    ax.set_aspect('equal', adjustable='box')
    
    cbar = plt.colorbar(scatter, ax=ax)
    cbar.set_label('Absolute Error', fontsize=11)
    
    plt.tight_layout()
    plt.savefig('/Users/pascalschlegel/effort-estimator/01_TRAIN_VS_TEST.png', dpi=300, bbox_inches='tight')
    print("âœ“ Saved: 01_TRAIN_VS_TEST.png")
    plt.close()

def plot_metrics_bars(data):
    """MAE, RMSE, RÂ² comparison bars"""
    y_train = data['y_train']
    y_test = data['y_test']
    y_train_pred = data['y_train_pred']
    y_test_pred = data['y_test_pred']
    
    metrics_data = {
        'RÂ² Score': [r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)],
        'MAE': [mean_absolute_error(y_train, y_train_pred), mean_absolute_error(y_test, y_test_pred)],
        'RMSE': [np.sqrt(mean_squared_error(y_train, y_train_pred)), 
                 np.sqrt(mean_squared_error(y_test, y_test_pred))],
    }
    
    fig, ax = plt.subplots(figsize=(12, 7))
    
    x = np.arange(len(metrics_data))
    width = 0.35
    
    train_vals = [metrics_data[m][0] for m in metrics_data.keys()]
    test_vals = [metrics_data[m][1] for m in metrics_data.keys()]
    
    bars1 = ax.bar(x - width/2, train_vals, width, label='Training', 
                   color='#2ecc71', edgecolor='black', linewidth=1.5, alpha=0.85)
    bars2 = ax.bar(x + width/2, test_vals, width, label='Test', 
                   color='#e74c3c', edgecolor='black', linewidth=1.5, alpha=0.85)
    
    # Add value labels
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    ax.set_ylabel('Score / Error Value', fontsize=12, fontweight='bold')
    ax.set_title('Model Performance: Training vs Test Set\n(Lower is better for MAE/RMSE, Higher is better for RÂ²)', 
                fontsize=14, fontweight='bold', pad=20)
    ax.set_xticks(x)
    ax.set_xticklabels(metrics_data.keys(), fontsize=12, fontweight='bold')
    ax.legend(fontsize=12, loc='upper right')
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig('/Users/pascalschlegel/effort-estimator/02_METRICS_BARS.png', dpi=300, bbox_inches='tight')
    print("âœ“ Saved: 02_METRICS_BARS.png")
    plt.close()

def plot_residuals_4panel(data):
    """4-panel residuals analysis"""
    y_test = data['y_test']
    y_test_pred = data['y_test_pred']
    residuals = y_test - y_test_pred
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Residuals vs Predicted
    ax = axes[0, 0]
    ax.scatter(y_test_pred, residuals, s=100, alpha=0.6, edgecolors='black', linewidth=0.5)
    ax.axhline(y=0, color='r', linestyle='--', lw=2.5, label='Zero Error')
    ax.axhline(y=residuals.std(), color='orange', linestyle=':', lw=2, label='Â±1 Std Dev')
    ax.axhline(y=-residuals.std(), color='orange', linestyle=':', lw=2)
    ax.set_xlabel('Predicted Borg', fontsize=11, fontweight='bold')
    ax.set_ylabel('Residuals (Actual - Predicted)', fontsize=11, fontweight='bold')
    ax.set_title('Residuals vs Predicted Values', fontsize=12, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    
    # 2. Residuals histogram
    ax = axes[0, 1]
    ax.hist(residuals, bins=15, color='skyblue', edgecolor='black', alpha=0.7)
    ax.axvline(x=0, color='r', linestyle='--', lw=2.5, label='Zero')
    ax.axvline(x=residuals.mean(), color='green', linestyle=':', lw=2, label=f'Mean: {residuals.mean():.4f}')
    ax.set_xlabel('Residuals', fontsize=11, fontweight='bold')
    ax.set_ylabel('Frequency', fontsize=11, fontweight='bold')
    ax.set_title(f'Error Distribution\nMean={residuals.mean():.4f}, Std={residuals.std():.4f}', 
                fontsize=12, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')
    
    # 3. Absolute error by actual
    ax = axes[1, 0]
    abs_residuals = np.abs(residuals)
    ax.scatter(y_test, abs_residuals, s=100, alpha=0.6, edgecolors='black', linewidth=0.5)
    ax.axhline(y=np.mean(abs_residuals), color='r', linestyle='--', lw=2, 
              label=f'Mean Error: {np.mean(abs_residuals):.4f}')
    ax.set_xlabel('Actual Borg', fontsize=11, fontweight='bold')
    ax.set_ylabel('|Residuals|', fontsize=11, fontweight='bold')
    ax.set_title('Absolute Error vs Actual Effort', fontsize=12, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    
    # 4. Error percentiles
    ax = axes[1, 1]
    sorted_errors = np.sort(abs_residuals)
    percentiles = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors) * 100
    ax.plot(sorted_errors, percentiles, lw=3, color='darkblue')
    
    for p in [25, 50, 75, 90, 95]:
        val = np.percentile(abs_residuals, p)
        ax.plot(val, p, 'ro', markersize=8)
        ax.text(val, p + 2, f'{p}th: {val:.3f}', fontsize=9, ha='center')
    
    ax.set_xlabel('Absolute Error (Borg Points)', fontsize=11, fontweight='bold')
    ax.set_ylabel('Percentile (%)', fontsize=11, fontweight='bold')
    ax.set_title('Cumulative Error Distribution', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.set_ylim([0, 105])
    
    plt.tight_layout()
    plt.savefig('/Users/pascalschlegel/effort-estimator/03_RESIDUALS_4PANEL.png', dpi=300, bbox_inches='tight')
    print("âœ“ Saved: 03_RESIDUALS_4PANEL.png")
    plt.close()

def plot_top_25_features(data):
    """Top 25 features by importance"""
    model = data['model']
    selected_cols = data['selected_cols']
    
    importance = model.get_booster().get_score(importance_type='weight')
    
    feature_importance = {}
    for fid, score in importance.items():
        idx = int(fid.split('f')[1])
        if idx < len(selected_cols):
            feature_importance[selected_cols[idx]] = score
    
    sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
    top_features = sorted_features[:25]
    
    names = [f[0].replace('_', ' ').title() for f in top_features]
    scores = [f[1] for f in top_features]
    
    # Get modalities for colors
    colors = []
    for feat, _ in top_features:
        modality = feat.split('_')[0]
        if modality == 'ppg':
            colors.append('#1f77b4')  # Blue
        elif modality == 'eda':
            colors.append('#ff7f0e')  # Orange
        else:  # acc
            colors.append('#2ca02c')  # Green
    
    fig, ax = plt.subplots(figsize=(14, 11))
    bars = ax.barh(range(len(names)), scores, color=colors, edgecolor='black', linewidth=1)
    
    # Add value labels
    for i, (bar, score) in enumerate(zip(bars, scores)):
        ax.text(score, i, f' {score:.0f}', va='center', fontsize=9, fontweight='bold')
    
    ax.set_yticks(range(len(names)))
    ax.set_yticklabels(names, fontsize=10)
    ax.set_xlabel('Importance Score (Tree Splits)', fontsize=12, fontweight='bold')
    ax.set_title('Top 25 Most Important Features for Borg Effort Prediction', 
                fontsize=14, fontweight='bold', pad=20)
    ax.invert_yaxis()
    ax.grid(True, alpha=0.3, axis='x')
    
    # Add legend
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='#1f77b4', edgecolor='black', label='PPG (Heart Rate)'),
        Patch(facecolor='#ff7f0e', edgecolor='black', label='EDA (Skin Conductivity)'),
        Patch(facecolor='#2ca02c', edgecolor='black', label='IMU (Acceleration)')
    ]
    ax.legend(handles=legend_elements, loc='lower right', fontsize=11)
    
    plt.tight_layout()
    plt.savefig('/Users/pascalschlegel/effort-estimator/04_TOP_25_FEATURES.png', dpi=300, bbox_inches='tight')
    print("âœ“ Saved: 04_TOP_25_FEATURES.png")
    plt.close()

def plot_error_distribution(data):
    """Error distribution with histogram and KDE"""
    y_test = data['y_test']
    y_test_pred = data['y_test_pred']
    errors = np.abs(y_test - y_test_pred)
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # Histogram with KDE
    ax = axes[0]
    ax.hist(errors, bins=15, density=True, alpha=0.7, color='skyblue', edgecolor='black', linewidth=1)
    
    from scipy import stats
    x_range = np.linspace(errors.min(), errors.max(), 100)
    kde = stats.gaussian_kde(errors)
    ax.plot(x_range, kde(x_range), 'r-', lw=2.5, label='KDE')
    ax.axvline(errors.mean(), color='green', linestyle='--', lw=2.5, label=f'Mean: {errors.mean():.4f}')
    ax.axvline(np.median(errors), color='orange', linestyle='--', lw=2.5, label=f'Median: {np.median(errors):.4f}')
    
    ax.set_xlabel('Absolute Error (Borg Points)', fontsize=11, fontweight='bold')
    ax.set_ylabel('Density', fontsize=11, fontweight='bold')
    ax.set_title('Error Distribution', fontsize=12, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')
    
    # Cumulative
    ax = axes[1]
    sorted_errors = np.sort(errors)
    cumulative = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors) * 100
    ax.plot(sorted_errors, cumulative, lw=3, color='darkblue')
    
    percentiles = [50, 75, 90, 95]
    for p in percentiles:
        val = np.percentile(errors, p)
        ax.plot(val, p, 'ro', markersize=8)
        ax.text(val, p + 2, f'{p}th: {val:.3f}', fontsize=9, ha='center')
    
    ax.set_xlabel('Absolute Error (Borg Points)', fontsize=11, fontweight='bold')
    ax.set_ylabel('Percentile (%)', fontsize=11, fontweight='bold')
    ax.set_title('Cumulative Error Distribution', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.set_ylim([0, 105])
    
    plt.tight_layout()
    plt.savefig('/Users/pascalschlegel/effort-estimator/05_ERROR_DISTRIBUTION.png', dpi=300, bbox_inches='tight')
    print("âœ“ Saved: 05_ERROR_DISTRIBUTION.png")
    plt.close()

def print_summary(data):
    """Print comprehensive summary"""
    y_train = data['y_train']
    y_test = data['y_test']
    y_train_pred = data['y_train_pred']
    y_test_pred = data['y_test_pred']
    saved_metrics = data.get('saved_metrics')
    
    print("\n" + "="*90)
    print("MODEL PERFORMANCE SUMMARY")
    print("="*90)
    
    if saved_metrics:
        print("\nðŸ”´ TRAINING SET (343 samples) - From actual training")
        print(f"  RÂ² Score:    {saved_metrics['train']['r2']:.4f}")
        print(f"  MAE:         {saved_metrics['train']['mae']:.4f}")
        print(f"  RMSE:        {saved_metrics['train']['rmse']:.4f}")
        
        print("\nðŸŸ¢ TEST SET (86 samples) - From actual training")
        print(f"  RÂ² Score:    {saved_metrics['test']['r2']:.4f}  âœ… Excellent!")
        print(f"  MAE:         {saved_metrics['test']['mae']:.4f} Borg points")
        print(f"  RMSE:        {saved_metrics['test']['rmse']:.4f} Borg points")
    else:
        print("\nðŸ”´ TRAINING SET (343 samples)")
        print(f"  RÂ² Score:    {r2_score(y_train, y_train_pred):.4f}")
        print(f"  MAE:         {mean_absolute_error(y_train, y_train_pred):.4f}")
        print(f"  RMSE:        {np.sqrt(mean_squared_error(y_train, y_train_pred)):.4f}")
        
        print("\nðŸŸ¢ TEST SET (86 samples)")
        print(f"  RÂ² Score:    {r2_score(y_test, y_test_pred):.4f}  âœ… Excellent!")
        print(f"  MAE:         {mean_absolute_error(y_test, y_test_pred):.4f} Borg points")
        print(f"  RMSE:        {np.sqrt(mean_squared_error(y_test, y_test_pred)):.4f} Borg points")
    
    print("\nðŸ“Š KEY INSIGHTS")
    print("  1. Top predictor: EDA skin conductance slope (nervous system response)")
    print("  2. Strong cardiovascular signals: PPG features (heart rate)")
    print("  3. Movement patterns: IMU acceleration metrics")
    print("  4. Model generalizes well: minimal overfitting")

def main():
    print("\n" + "="*90)
    print("ðŸš€ COMPREHENSIVE MODEL ANALYSIS - ALL-IN-ONE SCRIPT")
    print("="*90)
    
    print("\nðŸ“Š Loading model and data...")
    data = load_model_and_prepare()
    
    print("ðŸŽ¨ Generating visualizations...\n")
    plot_train_vs_test_with_diagonal(data)
    plot_metrics_bars(data)
    plot_residuals_4panel(data)
    plot_top_25_features(data)
    plot_error_distribution(data)
    
    print_summary(data)
    
    print("\n" + "="*90)
    print("âœ… ALL VISUALIZATIONS COMPLETE!")
    print("="*90)
    print("\nGenerated plots:")
    print("  1. 01_TRAIN_VS_TEST.png - Train vs Test with RED diagonal line")
    print("  2. 02_METRICS_BARS.png - RÂ², MAE, RMSE comparison")
    print("  3. 03_RESIDUALS_4PANEL.png - Comprehensive residual analysis")
    print("  4. 04_TOP_25_FEATURES.png - Top features by importance")
    print("  5. 05_ERROR_DISTRIBUTION.png - Error histogram and percentiles")
    print("\nAll plots saved to: /Users/pascalschlegel/effort-estimator/")
    print("="*90 + "\n")

if __name__ == "__main__":
    main()
