#!/usr/bin/env python3
"""
THESIS PRESENTATION: Detailed Chronological Narrative
Each slide elaborated with clear explanations
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_predict, LeaveOneGroupOut, KFold
from scipy.stats import pearsonr
from sklearn.metrics import mean_absolute_error, confusion_matrix

# Load data
df = pd.read_csv('/Users/pascalschlegel/data/interim/elderly_combined_5subj/all_5_elderly_5s.csv')

exclude_cols = ['subject', 'borg', 't_center', 'window_start', 'window_end', 'unix_time', 'Unnamed: 0', 'index']
feature_cols = [c for c in df.columns if c not in exclude_cols 
                and df[c].dtype in ['float64', 'int64']
                and df[c].notna().sum() > 100]
valid_features = [c for c in feature_cols if df[c].isna().mean() < 0.5]

df_model = df.dropna(subset=['borg'])[['subject', 'borg'] + valid_features].dropna()

X_raw = df_model[valid_features].values
y = df_model['borg'].values
groups = df_model['subject'].values

def to_cat(b):
    if b <= 2: return 0      # LOW
    elif b <= 4: return 1    # MODERATE  
    else: return 2           # HIGH

y_cat = np.array([to_cat(b) for b in y])
cat_names = ['LOW (0-2)', 'MODERATE (3-4)', 'HIGH (5+)']

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘        THESIS PRESENTATION: EFFORT ESTIMATION FROM WEARABLE SENSORS          â•‘
â•‘                        Detailed Narrative with Elaborations                  â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# ==============================================================================
# SLIDE 1: THE GOAL
# ==============================================================================
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SLIDE 1: RESEARCH GOAL                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

WHAT IS PERCEIVED EFFORT?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Perceived effort is how hard someone FEELS they are working.
  
  It's measured using the BORG SCALE (0-10):
  
    0-2  = LOW effort      â†’ "I could do this all day" (resting, slow walk)
    3-4  = MODERATE effort â†’ "I'm working but comfortable" (normal walking)
    5-10 = HIGH effort     â†’ "This is hard!" (stairs, fast walking)
  
  This is SUBJECTIVE - two people doing the same activity may report
  different Borg scores based on their fitness, health, and perception.

WHY ESTIMATE IT AUTOMATICALLY?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â€¢ Elderly users can't constantly be asked "How hard is this?"
  â€¢ Overexertion is dangerous for elderly populations
  â€¢ We want to monitor effort WITHOUT user input
  â€¢ Goal: Wearable sensors â†’ Automatic Borg prediction

THE DREAM:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Train a model on SOME people â†’ Deploy to ANY new person â†’ Works automatically
  
  This is called "cross-subject generalization" - the holy grail of
  wearable-based health monitoring.
""")

# ==============================================================================
# SLIDE 2: THE DATA
# ==============================================================================
print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SLIDE 2: DATA COLLECTION                                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PARTICIPANTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â€¢ 5 elderly subjects (labeled P1-P5)
  â€¢ Age range: Elderly population (65+)
  â€¢ Simulated daily activities in controlled setting

ACTIVITIES PERFORMED:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â€¢ Seated rest        â†’ Expected LOW effort (Borg 0-2)
  â€¢ Slow walking       â†’ Expected LOW-MODERATE effort (Borg 2-3)
  â€¢ Normal walking     â†’ Expected MODERATE effort (Borg 3-5)
  â€¢ Fast walking       â†’ Expected MODERATE-HIGH effort (Borg 4-6)
  â€¢ Stair climbing     â†’ Expected HIGH effort (Borg 5-8)
  
  After each activity segment, users reported their Borg score.

SENSORS USED:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. PPG (Photoplethysmography)
     â†’ Measures heart rate, heart rate variability
     â†’ Why: Heart works harder during effort
     
  2. EDA (Electrodermal Activity)
     â†’ Measures skin conductance (sweat response)
     â†’ Why: We sweat more when working hard
     
  3. IMU (Accelerometer + Gyroscope)
     â†’ Measures motion intensity and patterns
     â†’ Why: Faster/more intense movement = more effort

PREPROCESSING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â€¢ Window size: 5.0 seconds with 70% overlap
    (Each "sample" represents 5 seconds of sensor data)
    
  â€¢ {len(valid_features)} features extracted per window
    - PPG features: heart rate, HRV metrics, signal quality
    - EDA features: skin conductance level, stress indicators
    - IMU features: acceleration magnitude, movement patterns
    
  â€¢ Final dataset: {len(df_model)} labeled samples
""")

# ==============================================================================
# SLIDE 3: FIRST ATTEMPT
# ==============================================================================
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SLIDE 3: FIRST ATTEMPT - Cross-Subject Prediction                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

THE APPROACH: Leave-One-Subject-Out (LOSO) Cross-Validation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  What is LOSO?
  
    Round 1: Train on P1,P2,P3,P4 â†’ Test on P5
    Round 2: Train on P1,P2,P3,P5 â†’ Test on P4
    Round 3: Train on P1,P2,P4,P5 â†’ Test on P3
    Round 4: Train on P1,P3,P4,P5 â†’ Test on P2
    Round 5: Train on P2,P3,P4,P5 â†’ Test on P1
    
  This simulates deploying to a NEW person who wasn't in training.
  It's the HARDEST but most realistic test.

THE MODEL:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â€¢ Ridge Regression (linear model with regularization)
  â€¢ Input: 284 sensor features
  â€¢ Output: Predicted Borg score (0-10)
  
  Why Ridge? With 284 features and only 584 samples, we need
  regularization to prevent overfitting. Ridge worked better than
  complex models like XGBoost on this small dataset.
""")

# Run Method 1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_raw)
logo = LeaveOneGroupOut()
model = Ridge(alpha=1.0)
y_pred_1 = cross_val_predict(model, X_scaled, y, cv=logo, groups=groups)

r_1, _ = pearsonr(y, y_pred_1)
mae_1 = mean_absolute_error(y, y_pred_1)
y_pred_1_cat = np.array([to_cat(b) for b in y_pred_1])

# Calculate confusion
exact_1 = (y_cat == y_pred_1_cat).mean()
off_by_1 = (np.abs(y_cat - y_pred_1_cat) == 1).mean()
off_by_2 = (np.abs(y_cat - y_pred_1_cat) == 2).mean()
within_1_cat = (np.abs(y_cat - y_pred_1_cat) <= 1).mean()

# Confusion matrix
cm_1 = confusion_matrix(y_cat, y_pred_1_cat)

print(f"""
RESULTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  CONTINUOUS METRICS:
    â€¢ Pearson correlation (r) = {r_1:.2f}
    â€¢ Mean Absolute Error     = {mae_1:.2f} Borg points
    
  WHAT THIS MEANS:
    â€¢ r = 0.18 is WEAK correlation
    â€¢ MAE = 2.04 means predictions are off by ~2 Borg points on average
    â€¢ If true Borg is 5, model might predict 3 or 7

  CATEGORICAL ACCURACY (LOW / MODERATE / HIGH):
    â€¢ Exact category correct:     {exact_1:.1%}
    â€¢ Off by 1 category:          {off_by_1:.1%}
    â€¢ Off by 2 categories:        {off_by_2:.1%}  â† Confuses LOW with HIGH!
    
    â€¢ "Close enough" (within Â±1): {within_1_cat:.1%}
    
  CONFUSION MATRIX:
                      Predicted
                   LOW    MOD    HIGH
    Actual LOW    [{cm_1[0,0]:3d}]   {cm_1[0,1]:3d}     {cm_1[0,2]:3d}
    Actual MOD     {cm_1[1,0]:3d}   [{cm_1[1,1]:3d}]    {cm_1[1,2]:3d}
    Actual HIGH    {cm_1[2,0]:3d}    {cm_1[2,1]:3d}    [{cm_1[2,2]:3d}]

INTERPRETATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ˜Ÿ r = 0.18 is POOR - we can't accurately predict the exact Borg score
  
  ğŸ¤” BUT: Only {off_by_2:.1%} of predictions confuse LOW with HIGH!
     This means the model rarely makes DANGEROUS mistakes.
     
     For safety applications, this might be "good enough":
     - If someone is at HIGH effort, we won't tell them they're at LOW
     - We might be off by one level, but not catastrophically wrong
""")

# ==============================================================================
# SLIDE 4: WHY DOES IT FAIL?
# ==============================================================================
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SLIDE 4: INVESTIGATION - Why Does Cross-Subject Fail?                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

HYPOTHESIS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Different people have different:
    1. PHYSIOLOGICAL BASELINES (resting heart rate, skin conductance)
    2. SUBJECTIVE PERCEPTION (what "effort" means to them)
  
  Let's check if this is true in our data.

ANALYSIS: Baseline Differences Across Subjects
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
""")

# Show baseline differences
print(f"  Subject â”‚ Samples â”‚ Borg Range â”‚ Borg Mean â”‚ What this tells us")
print(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

for subj in sorted(df_model['subject'].unique()):
    mask = df_model['subject'] == subj
    n = sum(mask)
    borg_min = df_model.loc[mask, 'borg'].min()
    borg_max = df_model.loc[mask, 'borg'].max()
    borg_mean = df_model.loc[mask, 'borg'].mean()
    
    label = subj.replace('sim_elderly', 'P')
    
    if borg_mean < 2:
        note = "Rates everything LOW"
    elif borg_mean > 4:
        note = "Rates everything HIGH"
    else:
        note = "Average rater"
    
    print(f"  {label:7s} â”‚ {n:7d} â”‚ {borg_min:.0f} - {borg_max:.0f}     â”‚ {borg_mean:.2f}      â”‚ {note}")

print(f"""

KEY OBSERVATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â€¢ P5's mean Borg is 1.08 - they NEVER rate above LOW effort!
  â€¢ P3's and P4's mean is ~3.9 - they use the full scale
  â€¢ Same activities, DIFFERENT perception!

FEATURE BASELINE DIFFERENCES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Looking at EDA (skin conductance) - a key stress/effort indicator:
""")

# Get EDA feature if exists
eda_feat = None
for col in ['eda_stress_skin_mean', 'eda_tonic_mean', 'eda_scl_mean']:
    if col in df_model.columns:
        eda_feat = col
        break

if eda_feat:
    print(f"  Subject â”‚ EDA Mean    â”‚ EDA Range")
    print(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    for subj in sorted(df_model['subject'].unique()):
        mask = df_model['subject'] == subj
        eda_mean = df_model.loc[mask, eda_feat].mean()
        eda_min = df_model.loc[mask, eda_feat].min()
        eda_max = df_model.loc[mask, eda_feat].max()
        label = subj.replace('sim_elderly', 'P')
        print(f"  {label:7s} â”‚ {eda_mean:>10.1f}  â”‚ {eda_min:.0f} - {eda_max:.0f}")

print("""
  
  PROBLEM: EDA baselines are COMPLETELY different!
  
    â€¢ P3's EDA might range from 5-15
    â€¢ P1's EDA might range from 80-120
    
    An EDA value of "50" means NOTHING without knowing the person!
    Is 50 high for them? Low? We can't tell.
""")

# ==============================================================================
# SLIDE 5: THE INSIGHT
# ==============================================================================
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SLIDE 5: THE KEY INSIGHT - Simpson's Paradox                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

THE PROBLEM IS TWOFOLD:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  1. PHYSIOLOGICAL BASELINE DIFFERENCES
     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     Different people have different resting values:
     
     Person A: Resting HR = 60, Max HR = 120
     Person B: Resting HR = 80, Max HR = 140
     
     If we see HR = 100, is that high effort?
     â€¢ For Person A: YES (100 is near their max)
     â€¢ For Person B: NO (100 is only moderate for them)

  2. SUBJECTIVE PERCEPTION DIFFERENCES  
     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     "Borg 5" means different things to different people:
     
     Person A: Rates climbing stairs as Borg 3 ("not too bad")
     Person B: Rates climbing stairs as Borg 7 ("really hard!")
     
     Same physical activity, different subjective experience.

THIS IS CALLED: SIMPSON'S PARADOX
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Simpson's Paradox: A trend that appears when data is POOLED may
  disappear or REVERSE when data is looked at by group.
  
  EXAMPLE IN OUR DATA:
  
    Pooled data (all subjects together):
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      High EDA â†’ Medium Borg (r â‰ˆ 0.1)
      
      But this is FAKE! The model is just learning:
      "This looks like P1's data (high EDA) â†’ P1's typical Borg (medium)"
      
    Within each subject:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      High EDA â†’ High Borg (r â‰ˆ 0.4)
      
      THIS is the real physiological relationship!

  THE MODEL'S MISTAKE:
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Cross-subject model uses features to identify WHICH PERSON it is,
    not to measure ACTUAL EFFORT.
    
    It learns: "High EDA baseline = probably P1 = medium Borg"
    Instead of: "EDA increased from rest = increased effort"
""")

# ==============================================================================
# SLIDE 6: PROOF - Within-Subject Works
# ==============================================================================
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SLIDE 6: PROOF - Within-Subject Prediction Works!                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

IF OUR HYPOTHESIS IS CORRECT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  If the problem is individual differences, then predicting WITHIN the same
  person should work much better. Let's test this.

THE APPROACH: Within-Subject 5-Fold Cross-Validation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  For EACH subject separately:
    â€¢ Split their data into 5 parts (80% train, 20% test each fold)
    â€¢ Train a model on THEIR data
    â€¢ Test on THEIR held-out data
    
  This tests: "If we have data from a person, can we predict their effort?"
""")

# Run Method 4
within_results = []
within_predictions = {}

for subj in sorted(df_model['subject'].unique()):
    mask = df_model['subject'] == subj
    X_subj = X_raw[mask]
    y_subj = y[mask]
    
    scaler_subj = StandardScaler()
    X_subj_scaled = scaler_subj.fit_transform(X_subj)
    
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    y_pred_subj = cross_val_predict(model, X_subj_scaled, y_subj, cv=kf)
    
    r_subj, _ = pearsonr(y_subj, y_pred_subj)
    mae_subj = mean_absolute_error(y_subj, y_pred_subj)
    
    y_cat_subj = np.array([to_cat(b) for b in y_subj])
    y_pred_cat_subj = np.array([to_cat(b) for b in y_pred_subj])
    within_1 = (np.abs(y_cat_subj - y_pred_cat_subj) <= 1).mean()
    
    within_results.append({
        'subject': subj, 
        'r': r_subj, 
        'mae': mae_subj, 
        'within_1': within_1,
        'n': sum(mask)
    })

mean_r_within = np.mean([r['r'] for r in within_results])
mean_mae_within = np.mean([r['mae'] for r in within_results])
mean_within1 = np.mean([r['within_1'] for r in within_results])

print(f"""
RESULTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Subject â”‚ Samples â”‚ Correlation (r) â”‚ MAE (Borg) â”‚ Within Â±1 category
  â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€""")

for res in within_results:
    label = res['subject'].replace('sim_elderly', 'P')
    print(f"  {label:7s} â”‚ {res['n']:7d} â”‚ {res['r']:15.3f} â”‚ {res['mae']:10.2f} â”‚ {res['within_1']:17.1%}")

print(f"""  â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  MEAN    â”‚         â”‚ {mean_r_within:15.3f} â”‚ {mean_mae_within:10.2f} â”‚ {mean_within1:17.1%}

INTERPRETATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… Mean r = 0.67 - MUCH better than cross-subject (0.18)!
  âœ… MAE = 0.92 Borg points (vs 2.04 for cross-subject)
  âœ… 98% of predictions are within Â±1 category
  
  This PROVES our hypothesis:
    â€¢ Features DO correlate with effort WITHIN each person
    â€¢ The cross-subject failure is due to individual differences
    
  âŒ BUT: This requires training data from that specific person
     We can't deploy this to a new user without their data first.
""")

# ==============================================================================
# SLIDE 7: FAILED SOLUTION
# ==============================================================================
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SLIDE 7: FAILED SOLUTION - Normalize Features Only                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

THE IDEA:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  If the problem is baseline differences, let's REMOVE them!
  
  For each person, convert their features to Z-SCORES:
  
    z = (value - person's_mean) / person's_std
    
  Now instead of "EDA = 1300" we have "EDA = 1.5Ïƒ above MY baseline"
  
  Everyone's features are now on the same scale!

THE HOPE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  "With normalized features, the model can learn:
   'Features 1Ïƒ above baseline â†’ Borg 5'
   regardless of who the person is."
""")

# Run Method 2
df_norm = df_model.copy()
for feat in valid_features:
    for subj in df_model['subject'].unique():
        mask = df_model['subject'] == subj
        subj_mean = df_model.loc[mask, feat].mean()
        subj_std = df_model.loc[mask, feat].std()
        if subj_std > 0:
            df_norm.loc[mask, feat] = (df_model.loc[mask, feat] - subj_mean) / subj_std
        else:
            df_norm.loc[mask, feat] = 0

X_norm = df_norm[valid_features].values

y_pred_2 = cross_val_predict(model, X_norm, y, cv=logo, groups=groups)
r_2, _ = pearsonr(y, y_pred_2)
mae_2 = mean_absolute_error(y, y_pred_2)
y_pred_2_cat = np.array([to_cat(b) for b in y_pred_2])
within_1_cat_2 = (np.abs(y_cat - y_pred_2_cat) <= 1).mean()

print(f"""
RESULT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Pearson correlation (r) = {r_2:.2f}         â”‚
  â”‚  Mean Absolute Error     = {mae_2:.2f} Borg   â”‚
  â”‚  Within Â±1 category      = {within_1_cat_2:.0%}        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  ğŸ˜± IT'S WORSE! r = 0.05 vs r = 0.18 before!

WHY DID THIS FAIL?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  
  We normalized FEATURES but not the TARGET (Borg)!
  
  BEFORE (raw features):
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Model input:  "EDA = 1300" 
    Model learns: "That looks like P1 â†’ predict P1's typical Borg (â‰ˆ3)"
    
    This is WRONG but at least gave SOME signal (r = 0.18)
    
  AFTER (normalized features):
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Model input:  "EDA = +1.2Ïƒ above their baseline"
    Model output: "Predict Borg = ???"
    
    The model has NO ANCHOR:
    â€¢ +1.2Ïƒ for P1 might mean Borg 4
    â€¢ +1.2Ïƒ for P5 might mean Borg 2
    
    We removed the only signal (person identity) without replacing it!

THE ANALOGY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  
  It's like converting temperatures to "deviation from your city's average"
  and then asking "what's the actual temperature?"
  
    Input:  "It's 2Ïƒ warmer than average in your city"
    Output: "Is that 60Â°F or 90Â°F?"  â† Can't tell without knowing the city!
    
  RELATIVE features cannot predict ABSOLUTE targets.
""")

# ==============================================================================
# SLIDE 8: THE SOLUTION
# ==============================================================================
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SLIDE 8: THE SOLUTION - Normalize BOTH Features AND Borg                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

THE KEY INSIGHT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  
  If features are RELATIVE, the target must also be RELATIVE!
  
  Instead of predicting absolute Borg (0-10), predict RELATIVE Borg:
  "How much above/below their personal average?"

THE APPROACH:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  
  Step 1: Normalize features (same as before)
          z_feature = (feature - person_mean) / person_std
          
  Step 2: Normalize Borg target
          z_borg = (borg - person_mean_borg) / person_std_borg
          
  Step 3: Train model to predict z_borg from z_features
          This learns: "Features 1Ïƒ up â†’ Borg 0.5Ïƒ up"
          
  Step 4: To get actual Borg prediction, denormalize:
          predicted_borg = z_pred Ã— person_std_borg + person_mean_borg

WHAT THIS REQUIRES - CALIBRATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  
  To denormalize, we need to know each person's:
    â€¢ Mean Borg (their "typical" effort level)
    â€¢ Std Borg (their range of effort levels)
    â€¢ Mean/Std of their features (their baseline)
    
  This requires a CALIBRATION phase with ~20 labeled samples.
""")

# Run Method 3
df_norm['borg_norm'] = 0.0
borg_stats = {}
for subj in df_model['subject'].unique():
    mask = df_model['subject'] == subj
    subj_mean = df_model.loc[mask, 'borg'].mean()
    subj_std = df_model.loc[mask, 'borg'].std()
    borg_stats[subj] = {'mean': subj_mean, 'std': subj_std}
    if subj_std > 0:
        df_norm.loc[mask, 'borg_norm'] = (df_model.loc[mask, 'borg'] - subj_mean) / subj_std

y_norm = df_norm['borg_norm'].values

y_pred_3_norm = cross_val_predict(model, X_norm, y_norm, cv=logo, groups=groups)

y_pred_3 = np.zeros_like(y_pred_3_norm)
for subj in df_model['subject'].unique():
    mask = groups == subj
    y_pred_3[mask] = y_pred_3_norm[mask] * borg_stats[subj]['std'] + borg_stats[subj]['mean']

r_3, _ = pearsonr(y, y_pred_3)
mae_3 = mean_absolute_error(y, y_pred_3)
y_pred_3_cat = np.array([to_cat(b) for b in y_pred_3])
within_1_cat_3 = (np.abs(y_cat - y_pred_3_cat) <= 1).mean()
exact_3 = (y_cat == y_pred_3_cat).mean()

print(f"""
RESULTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Pearson correlation (r) = {r_3:.2f}         â”‚
  â”‚  Mean Absolute Error     = {mae_3:.2f} Borg   â”‚
  â”‚  Within Â±1 category      = {within_1_cat_3:.0%}        â”‚
  â”‚  Exact category          = {exact_3:.0%}        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  ğŸ‰ HUGE IMPROVEMENT! r = 0.61 (vs 0.18 raw, vs 0.05 features-only)

WHY THIS WORKS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  The model now learns a UNIVERSAL relationship:
  
    "When features are 1Ïƒ above YOUR baseline,
     Borg tends to be 0.5Ïƒ above YOUR baseline"
     
  This relationship is the SAME for everyone because:
    â€¢ We removed individual baseline differences (normalization)
    â€¢ We're predicting relative change, not absolute values
    â€¢ The mapping between relative-feature â†’ relative-effort transfers!

  The calibration data provides the "anchor" to convert back to absolute Borg.

COMPARISON TO WITHIN-SUBJECT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â€¢ Within-subject r = 0.67 (requires ALL their data for training)
  â€¢ Calibrated cross-subject r = 0.61 (requires only ~20 samples!)
  
  We get 91% of the within-subject performance with just calibration!
""")

# ==============================================================================
# SLIDE 9: COMPARISON
# ==============================================================================
print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SLIDE 9: SUMMARY COMPARISON - All Methods                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method                                   â”‚ r      â”‚ MAE    â”‚ Within Â±1 cat  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Cross-subject (raw features)          â”‚ {r_1:.2f}   â”‚ {mae_1:.2f}   â”‚ {within_1_cat:.0%}           â”‚
â”‚ 2. Cross-subject (features normalized)   â”‚ {r_2:.2f}   â”‚ {mae_2:.2f}   â”‚ {within_1_cat_2:.0%}           â”‚
â”‚ 3. Cross-subject WITH CALIBRATION        â”‚ {r_3:.2f}   â”‚ {mae_3:.2f}   â”‚ {within_1_cat_3:.0%}           â”‚
â”‚ 4. Within-subject (personal model)       â”‚ {mean_r_within:.2f}   â”‚ {mean_mae_within:.2f}   â”‚ {mean_within1:.0%}           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

VISUAL COMPARISON:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Correlation (r):
  
    Method 1 (raw):        â–ˆâ–ˆâ–ˆâ–ˆ                          r = 0.18
    Method 2 (feat norm):  â–ˆ                             r = 0.05
    Method 3 (CALIBRATED): â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     r = 0.61
    Method 4 (within):     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    r = 0.67
                           0.0           0.5           1.0

KEY TAKEAWAYS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  1. Raw cross-subject is POOR (r=0.18)
     â†’ Baselines too different, model learns person identity not effort
     
  2. Normalizing features alone makes it WORSE (r=0.05)
     â†’ Relative features can't predict absolute targets
     
  3. Calibration nearly matches within-subject (r=0.61 vs 0.67)
     â†’ Brief calibration unlocks personalized prediction
     
  4. ~8 minutes of calibration gives 91% of maximum performance!
""")

# ==============================================================================
# SLIDE 10: PRACTICAL IMPLEMENTATION
# ==============================================================================
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SLIDE 10: PRACTICAL IMPLEMENTATION - Calibration Protocol                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

WHAT IS CALIBRATION?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  A short session where a new user performs activities while providing
  Borg ratings. This gives us their personal baselines.

PROPOSED CALIBRATION PROTOCOL (~8 minutes total):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Activity        â”‚ Duration â”‚ Expected Effort â”‚ Borg Ratings
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Seated rest     â”‚ 2 min    â”‚ LOW             â”‚ 2 ratings
  Slow walking    â”‚ 2 min    â”‚ LOW-MODERATE    â”‚ 2 ratings
  Normal walking  â”‚ 2 min    â”‚ MODERATE        â”‚ 2 ratings
  Fast walking    â”‚ 2 min    â”‚ MODERATE-HIGH   â”‚ 2 ratings
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL           â”‚ 8 min    â”‚ Covers range    â”‚ ~8 ratings

WHY THIS WORKS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  With 8 minutes of data, we get:
  
    â€¢ ~100 sensor windows (5-second windows, 70% overlap)
    â€¢ 8 Borg ratings covering LOW â†’ HIGH effort
    
  From this, we extract:
  
    â€¢ Feature baselines: Mean and std of each feature for this person
    â€¢ Borg baselines: Mean and std of their Borg ratings
    
  This is enough to anchor the model to their personal scale!

DEPLOYMENT WORKFLOW:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  DAY 1: CALIBRATION                                                 â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
  â”‚    1. User wears sensors                                            â”‚
  â”‚    2. User does 8-min calibration protocol                          â”‚
  â”‚    3. System computes personal baseline statistics                  â”‚
  â”‚    4. User is ready for autonomous monitoring!                      â”‚
  â”‚                                                                     â”‚
  â”‚  DAY 2+: AUTONOMOUS PREDICTION                                      â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
  â”‚    1. User wears sensors, goes about their day                      â”‚
  â”‚    2. System continuously records sensor data                       â”‚
  â”‚    3. For each window:                                              â”‚
  â”‚       a. Normalize features using their baseline                    â”‚
  â”‚       b. Apply cross-subject model â†’ get relative Borg prediction   â”‚
  â”‚       c. Denormalize using their Borg baseline â†’ absolute Borg      â”‚
  â”‚    4. Alert if effort is too HIGH for too long                      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

# ==============================================================================
# SLIDE 11: CONCLUSIONS
# ==============================================================================
print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SLIDE 11: CONCLUSIONS AND CONTRIBUTIONS                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MAIN FINDINGS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  1. CROSS-SUBJECT EFFORT ESTIMATION IS HARD
     â€¢ Achieved r = 0.18 with raw features
     â€¢ Reason: Individual physiological baselines + subjective perception
     â€¢ Cannot deploy "out of the box" to new users
     
  2. WITHIN-SUBJECT ESTIMATION WORKS WELL
     â€¢ Achieved r = 0.67 with personal models
     â€¢ Proves that features DO capture effort physiology
     â€¢ But requires training data from that specific person
     
  3. CALIBRATION BRIDGES THE GAP
     â€¢ Achieved r = 0.61 with ~8 min calibration
     â€¢ Key insight: Normalize BOTH features AND targets
     â€¢ Gets 91% of within-subject performance
     
  4. CATEGORICAL ACCURACY IS HIGH EVEN WITHOUT CALIBRATION
     â€¢ 87% of raw predictions within Â±1 effort category
     â€¢ Useful for safety: rarely confuses LOW with HIGH

SCIENTIFIC CONTRIBUTION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â€¢ Demonstrated that perceived effort is INHERENTLY PERSONAL
    â†’ Cannot be predicted cross-subject without calibration
    â†’ This is a fundamental limitation, not a model failure
    
  â€¢ Identified Simpson's Paradox in effort estimation
    â†’ Pooled correlations are misleading
    â†’ Within-subject analysis reveals true relationships
    
  â€¢ Proposed practical calibration protocol
    â†’ 8 minutes enables personalized prediction
    â†’ Feasible for real-world deployment

PRACTICAL CONTRIBUTION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â€¢ Developed wearable-based effort estimation pipeline
  â€¢ Achieved r = 0.61 correlation with practical calibration
  â€¢ 95% of predictions within Â±1 effort category with calibration
  â€¢ Ready for longitudinal validation studies

FUTURE WORK:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  1. LONGITUDINAL VALIDATION
     â€¢ Does calibration hold over days/weeks?
     â€¢ How often does re-calibration need to occur?
     
  2. MINIMUM CALIBRATION
     â€¢ Can we reduce from 8 minutes to 3 minutes?
     â€¢ What's the minimum data needed?
     
  3. TRANSFER LEARNING
     â€¢ With more subjects, can we reduce calibration needs?
     â€¢ Can we predict baseline from demographics?

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  THESIS SUMMARY STATEMENT:
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  "Cross-subject perceived effort estimation fails (r=0.18) due to 
   individual baseline differences and subjective perception. 
   
   However, with a brief ~8-minute calibration phase, prediction 
   improves dramatically to r=0.61 by learning each person's baseline.
   
   This demonstrates that perceived effort is inherently personal,
   motivating a longitudinal personalized approach for deployment."

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
""")

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                           THANK YOU - QUESTIONS?                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
