#!/usr/bin/env python3
"""
Categorical Effort Analysis: LOW / MODERATE / HIGH

This script shows that while exact Borg prediction has MAE=2,
the practical utility for categorical classification is much better.

Borg Categories:
- LOW:      0-2 (very light to light)
- MODERATE: 3-4 (moderate)  
- HIGH:     5-10 (hard to maximal)

This is the clinically relevant metric for most applications.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_predict, LeaveOneGroupOut
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from scipy.stats import pearsonr

# =============================================================================
# CONFIGURATION
# =============================================================================
DATA_PATH = '/Users/pascalschlegel/data/interim/elderly_combined_5subj/all_5_elderly_5s.csv'
OUTPUT_DIR = '/Users/pascalschlegel/data/interim/elderly_combined_5subj/ml_expert_plots'

# Borg category thresholds (clinically meaningful)
def borg_to_category(borg):
    """Convert Borg CR-10 to LOW/MODERATE/HIGH"""
    if borg <= 2:
        return 'LOW'
    elif borg <= 4:
        return 'MODERATE'
    else:
        return 'HIGH'

def borg_to_numeric(borg):
    """Convert Borg CR-10 to 0/1/2 for confusion matrix"""
    if borg <= 2:
        return 0  # LOW
    elif borg <= 4:
        return 1  # MODERATE
    else:
        return 2  # HIGH

# =============================================================================
# LOAD AND PREPARE DATA
# =============================================================================
print("="*70)
print("CATEGORICAL EFFORT ANALYSIS: LOW / MODERATE / HIGH")
print("="*70)

df = pd.read_csv(DATA_PATH)

# Feature selection - include HR features explicitly
exclude_cols = ['subject', 'borg', 't_center', 'window_start', 'window_end', 
                'unix_time', 'Unnamed: 0', 'index']
feature_cols = [c for c in df.columns if c not in exclude_cols 
                and df[c].dtype in ['float64', 'int64']
                and df[c].notna().sum() > 100]

# Check HR feature availability
hr_features = [c for c in feature_cols if 'hr' in c.lower()]
print(f"\nâœ“ HR features available: {len(hr_features)}")
print(f"  Examples: {hr_features[:5]}")

hrv_features = [c for c in feature_cols if any(x in c.lower() for x in ['rmssd', 'sdnn'])]
print(f"âœ“ HRV features available: {len(hrv_features)}")

# Clean data
df_clean = df.dropna(subset=['borg'])
valid_features = [c for c in feature_cols if df_clean[c].isna().mean() < 0.5]
df_model = df_clean[['subject', 'borg'] + valid_features].dropna()

print(f"\nðŸ“Š Dataset: {len(df_model)} samples, {len(valid_features)} features")

# Prepare for modeling
X = df_model[valid_features].values
y = df_model['borg'].values
groups = df_model['subject'].values

# Standardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# =============================================================================
# RUN LOSO PREDICTION
# =============================================================================
print("\n" + "="*70)
print("RUNNING LOSO PREDICTION")
print("="*70)

logo = LeaveOneGroupOut()
model = Ridge(alpha=1.0)
y_pred = cross_val_predict(model, X_scaled, y, cv=logo, groups=groups)

# Continuous metrics
r, _ = pearsonr(y, y_pred)
mae = np.mean(np.abs(y - y_pred))
print(f"\nContinuous Metrics:")
print(f"  Pearson r: {r:.3f}")
print(f"  MAE: {mae:.2f} Borg units")

# =============================================================================
# CONVERT TO CATEGORIES
# =============================================================================
print("\n" + "="*70)
print("CATEGORICAL ANALYSIS")
print("="*70)

y_cat_true = np.array([borg_to_numeric(b) for b in y])
y_cat_pred = np.array([borg_to_numeric(b) for b in y_pred])

# Accuracy
accuracy = accuracy_score(y_cat_true, y_cat_pred)
print(f"\nðŸŽ¯ 3-CLASS ACCURACY: {accuracy:.1%}")

# Per-class accuracy
print("\nPer-class accuracy:")
for i, cat in enumerate(['LOW (0-2)', 'MODERATE (3-4)', 'HIGH (5+)']):
    mask = y_cat_true == i
    if mask.sum() > 0:
        acc = (y_cat_pred[mask] == i).mean()
        print(f"  {cat}: {acc:.1%} ({mask.sum()} samples)")

# Classification report
print("\nClassification Report:")
print(classification_report(y_cat_true, y_cat_pred, 
                           target_names=['LOW (0-2)', 'MODERATE (3-4)', 'HIGH (5+)']))

# Adjacent accuracy (within 1 category)
adjacent_correct = np.abs(y_cat_true - y_cat_pred) <= 1
adjacent_accuracy = adjacent_correct.mean()
print(f"\nðŸŽ¯ ADJACENT ACCURACY (Â±1 category): {adjacent_accuracy:.1%}")

# =============================================================================
# PER-SUBJECT CATEGORICAL ANALYSIS
# =============================================================================
print("\n" + "="*70)
print("PER-SUBJECT CATEGORICAL PERFORMANCE")
print("="*70)

SUBJECT_LABELS = {
    'sim_elderly1': 'P1', 'sim_elderly2': 'P2', 'sim_elderly3': 'P3',
    'sim_elderly4': 'P4', 'sim_elderly5': 'P5'
}

subject_results = {}
for subj in np.unique(groups):
    mask = groups == subj
    y_true_s = y_cat_true[mask]
    y_pred_s = y_cat_pred[mask]
    
    acc = accuracy_score(y_true_s, y_pred_s)
    adj_acc = (np.abs(y_true_s - y_pred_s) <= 1).mean()
    
    subject_results[subj] = {'accuracy': acc, 'adjacent': adj_acc, 'n': mask.sum()}
    print(f"  {SUBJECT_LABELS[subj]}: Exact={acc:.1%}, Adjacent={adj_acc:.1%} (n={mask.sum()})")

# =============================================================================
# VISUALIZATIONS
# =============================================================================
print("\n" + "="*70)
print("GENERATING CATEGORICAL PLOTS")
print("="*70)

plt.style.use('seaborn-v0_8-whitegrid')
SUBJECT_COLORS = {
    'sim_elderly1': '#E69F00', 'sim_elderly2': '#56B4E9', 'sim_elderly3': '#009E73',
    'sim_elderly4': '#CC79A7', 'sim_elderly5': '#F0E442'
}

# Plot 1: Confusion Matrix
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

ax1 = axes[0]
cm = confusion_matrix(y_cat_true, y_cat_pred)
cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

sns.heatmap(cm_pct, annot=True, fmt='.1f', cmap='Blues', ax=ax1,
            xticklabels=['LOW\n(0-2)', 'MOD\n(3-4)', 'HIGH\n(5+)'],
            yticklabels=['LOW\n(0-2)', 'MOD\n(3-4)', 'HIGH\n(5+)'])
ax1.set_xlabel('Predicted Category', fontsize=12)
ax1.set_ylabel('Actual Category', fontsize=12)
ax1.set_title(f'A. Confusion Matrix (%)\nOverall Accuracy: {accuracy:.1%}', 
              fontweight='bold', fontsize=13)

# Annotation with raw counts
for i in range(3):
    for j in range(3):
        ax1.text(j+0.5, i+0.7, f'(n={cm[i,j]})', ha='center', va='center', 
                fontsize=8, color='gray')

# Plot 2: Per-subject accuracy
ax2 = axes[1]
subjects = sorted(subject_results.keys())
x = np.arange(len(subjects))
width = 0.35

exact_accs = [subject_results[s]['accuracy'] for s in subjects]
adj_accs = [subject_results[s]['adjacent'] for s in subjects]

bars1 = ax2.bar(x - width/2, [a*100 for a in exact_accs], width, 
                label='Exact Match', color='steelblue', edgecolor='black')
bars2 = ax2.bar(x + width/2, [a*100 for a in adj_accs], width, 
                label='Adjacent (Â±1)', color='coral', edgecolor='black')

ax2.set_ylabel('Accuracy (%)', fontsize=12)
ax2.set_xlabel('Subject', fontsize=12)
ax2.set_title('B. Per-Subject Categorical Accuracy', fontweight='bold', fontsize=13)
ax2.set_xticks(x)
ax2.set_xticklabels([SUBJECT_LABELS[s] for s in subjects])
ax2.legend()
ax2.set_ylim(0, 100)
ax2.axhline(y=33.3, color='gray', linestyle='--', linewidth=1, label='Random (33%)')

# Add value labels
for bar in bars1:
    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
             f'{bar.get_height():.0f}%', ha='center', fontsize=9)
for bar in bars2:
    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
             f'{bar.get_height():.0f}%', ha='center', fontsize=9)

plt.tight_layout()
plt.savefig(f'{OUTPUT_DIR}/19_categorical_confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.close()
print("âœ“ 19_categorical_confusion_matrix.png")

# Plot 3: Practical Interpretation Figure
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# A: Borg scale explanation
ax1 = axes[0, 0]
ax1.axis('off')

borg_explanation = """
BORG CR-10 SCALE INTERPRETATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Rating   Description        Category
â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€
  0      Nothing at all     
  0.5    Extremely weak     LOW
  1      Very weak          (Rest/Light ADL)
  2      Weak (light)       
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  3      Moderate           MODERATE
  4      Somewhat strong    (Walking/Chores)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  5      Strong (heavy)     
  6                         HIGH
  7      Very strong        (Exercise/Exertion)
  8      
  9      
  10     Extremely strong   

KEY INSIGHT:
For practical applications, distinguishing
LOW vs MODERATE vs HIGH effort is sufficient.
Exact Borg values are inherently subjective.
"""
ax1.text(0.05, 0.95, borg_explanation, transform=ax1.transAxes, fontsize=10,
         verticalalignment='top', fontfamily='monospace',
         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.9))

# B: Histogram of predictions by true category
ax2 = axes[0, 1]
colors_cat = ['#2ecc71', '#f39c12', '#e74c3c']
labels_cat = ['LOW (0-2)', 'MODERATE (3-4)', 'HIGH (5+)']

for i, (label, color) in enumerate(zip(labels_cat, colors_cat)):
    mask = y_cat_true == i
    ax2.hist(y_pred[mask], bins=20, alpha=0.5, label=label, color=color, density=True)

ax2.set_xlabel('Predicted Borg CR-10', fontsize=11)
ax2.set_ylabel('Density', fontsize=11)
ax2.set_title('B. Prediction Distribution by True Category', fontweight='bold')
ax2.legend()
ax2.axvline(x=2, color='black', linestyle='--', linewidth=1)
ax2.axvline(x=4, color='black', linestyle='--', linewidth=1)
ax2.text(1, ax2.get_ylim()[1]*0.9, 'LOW', ha='center', fontsize=10)
ax2.text(3, ax2.get_ylim()[1]*0.9, 'MOD', ha='center', fontsize=10)
ax2.text(5.5, ax2.get_ylim()[1]*0.9, 'HIGH', ha='center', fontsize=10)

# C: Error distribution
ax3 = axes[1, 0]
errors = y - y_pred
ax3.hist(errors, bins=30, color='steelblue', edgecolor='black', alpha=0.7)
ax3.axvline(x=-2, color='red', linestyle='--', linewidth=2, label='Â±2 Borg')
ax3.axvline(x=2, color='red', linestyle='--', linewidth=2)
ax3.axvline(x=0, color='green', linestyle='-', linewidth=2, label='Perfect')

# Calculate percentage within Â±2
within_2 = (np.abs(errors) <= 2).mean() * 100
ax3.set_xlabel('Prediction Error (Actual - Predicted)', fontsize=11)
ax3.set_ylabel('Frequency', fontsize=11)
ax3.set_title(f'C. Error Distribution ({within_2:.0f}% within Â±2 Borg)', fontweight='bold')
ax3.legend()

# D: Summary statistics
ax4 = axes[1, 1]
ax4.axis('off')

summary_text = f"""
PRACTICAL PERFORMANCE SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CONTINUOUS METRICS:
  â€¢ Pearson r = {r:.2f}
  â€¢ MAE = {mae:.2f} Borg units
  â€¢ {within_2:.0f}% of predictions within Â±2 Borg

CATEGORICAL METRICS (3-class):
  â€¢ Exact accuracy = {accuracy:.1%}
  â€¢ Adjacent accuracy = {adjacent_accuracy:.1%}
    (correct or Â±1 category)

INTERPRETATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAE of 2 Borg on a 0-10 scale means:
  â€¢ ~80% of time within 1 category
  â€¢ Sufficient for LOW/MODERATE/HIGH
  â€¢ Clinical utility maintained

PRACTICAL APPLICATIONS:
  âœ“ Activity intensity monitoring
  âœ“ Exercise prescription adherence
  âœ“ Fatigue detection (HIGH effort)
  âœ“ Recovery monitoring (LOW effort)

NOT SUITABLE FOR:
  âœ— Precise Borg rating prediction
  âœ— Cross-subject deployment
  âœ— Without personalization
"""

ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, fontsize=10,
         verticalalignment='top', fontfamily='monospace',
         bbox=dict(boxstyle='round', facecolor='lightcyan', alpha=0.9))

plt.suptitle('Categorical Effort Classification: Practical Interpretation', 
             fontweight='bold', fontsize=14, y=0.98)
plt.tight_layout()
plt.savefig(f'{OUTPUT_DIR}/20_categorical_practical_interpretation.png', dpi=150, bbox_inches='tight')
plt.close()
print("âœ“ 20_categorical_practical_interpretation.png")

# Plot 4: Activity-level analysis (if we had activity labels)
fig, ax = plt.subplots(figsize=(10, 6))

# Show the practical use case
borg_bins = [0, 2, 4, 10]
borg_labels = ['LOW\n(Rest/Light)', 'MODERATE\n(Activity)', 'HIGH\n(Exertion)']

true_counts = np.histogram(y, bins=borg_bins)[0]
pred_counts = np.histogram(y_pred, bins=borg_bins)[0]

x = np.arange(len(borg_labels))
width = 0.35

bars1 = ax.bar(x - width/2, true_counts, width, label='Actual', color='#2ecc71', edgecolor='black')
bars2 = ax.bar(x + width/2, pred_counts, width, label='Predicted', color='#3498db', edgecolor='black')

ax.set_ylabel('Number of Windows', fontsize=12)
ax.set_xlabel('Effort Category', fontsize=12)
ax.set_title('Distribution of Effort Categories: Actual vs Predicted', fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(borg_labels)
ax.legend()

# Add value labels
for bar in bars1:
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, 
            f'{int(bar.get_height())}', ha='center', fontsize=11, fontweight='bold')
for bar in bars2:
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, 
            f'{int(bar.get_height())}', ha='center', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.savefig(f'{OUTPUT_DIR}/21_category_distribution.png', dpi=150, bbox_inches='tight')
plt.close()
print("âœ“ 21_category_distribution.png")

# =============================================================================
# FINAL SUMMARY
# =============================================================================
print("\n" + "="*70)
print("FINAL SUMMARY")
print("="*70)

print(f"""
KEY RESULTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Continuous:  r = {r:.2f}, MAE = {mae:.2f} Borg

Categorical (LOW/MODERATE/HIGH):
  â€¢ Exact match:     {accuracy:.1%}
  â€¢ Adjacent (Â±1):   {adjacent_accuracy:.1%}
  â€¢ Within Â±2 Borg:  {within_2:.0f}%

INTERPRETATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
While r = {r:.2f} looks "bad" for exact prediction,
the model correctly identifies effort CATEGORY {adjacent_accuracy:.0%} of the time.

For practical applications (activity monitoring, fatigue detection),
knowing LOW vs MODERATE vs HIGH is what matters - not exact Borg.

This supports your longitudinal approach:
  â€¢ Cross-subject: {accuracy:.0%} exact, {adjacent_accuracy:.0%} adjacent
  â€¢ With personalization: expect significantly better

PLOTS SAVED:
  â€¢ 19_categorical_confusion_matrix.png
  â€¢ 20_categorical_practical_interpretation.png  
  â€¢ 21_category_distribution.png
""")
